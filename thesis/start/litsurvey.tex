%literature survey
The essence of the oracle problem is best captured in the definition of non-testable programs, provided by Weyuker:
\begin{quote}
A program should be considered non-testable [if] (1) there does not exist an oracle; (2) it is theoretically possible, but practically too difficult to determine the correct output~\cite{Chays:2000:FTD:347636.348954}
\end{quote}
There are a variety of reasons why the oracle does not exist or can be exercised in a practical capacity. Weyuker describes that some software systems are like magic calculators – they are meant to inform us of the answer. She additionally mentions that some programs produce output that is impossible to read, either because of its volume or complexity. Here lies the core of software testing challenges. It is impossible to solve for every oracle or be able to describe every detail of a complex oracle – every system is entirely different and no generic template could create a standardized oracle. However, it is possible to avoid oracles altogether, generate them partially, or give hints about what details are most important to check.

One method of avoiding oracles altogether is metamorphic testing, which exploits the relationships of different executions of different input data. For example, upon white box inspection of a system, testers can see that outputs should relate directly to their inputs. They may not know much else about the system, or what its output means, but they know what differences they should see upon two different executions . Upon a certain execution they get f(5) = 20, and since they know the input-output relates directly, then they should predict that f(6) > f(5). Though exceptional in avoiding oracles, ascertaining metamorphic relationships is as challenging as the oracle problem.

Other researchers have proposed methods that do not attempt to provide or calculate an oracle, but rather aid the tester, informed of the structure of the system, exactly what details she should watch and constrain about the oracle. This takes the form of determining, through a series of tests, which variables in the system are most effective in revealing faults or bad behavior, and then providing them to the tester to define the ``correct'' value. One research group used mutant generation to find which variables killed the most mutants and, therefore, found the most faults . An alternative solution identified chains of dependent variables using probabilistic substitution graphs to find the most important variables . These works make the best advances in the reducing the human effort needed to define and fulfill a complicated oracle.

One group from Columbia University developed a similar approach when testing Machine Learning models, called ``parameterized random data generation''. They identified equivalence classes in their test data, which they then used as constraints for random data. This allowed them to develop huge data sets that still followed patterns that their model expects. They made a significant advancement in test data generation by adding more specification to what they want in their test data than just specifying data type or range of values. However, their approach is designed for Machine learning applications, so the values produced are restricted to positive integers only. Moreover, they have only used uniform distribution as their random selector.

Another group used Perlin noise as a data generator for images representing the spread of non-native pests in British forests. They layered multiple realistic images together and used spatial statistics to ``optimize'' their images. In other words, they identified traits from real test images, like a histogram of pixel values, and applied them to their own images to enforce similar traits. Most importantly, their tool generated images extremely efficiently and therefore quickly, making their tool exceptionally streamlined. However, their tool did not aim to provide any oracle information, and instead opted to using metamorphic relationships between real images and their fabricated images to find faults.

One group with members in Brazil and Luxembourg used their previous work on a method that identified how close test data approached synthetic bugs to create a new heuristic that ``finds'' data points that could reliably strongly kill the mutants in a program. Their approach works backwards by using a search scheme to identify data points at potential test candidates.

However, a more focused approach on database testing frameworks shows similarities to my own project. Generating test data for database-driven applications is still a niche research area and therefore lacking in advancement. One paper described the typical approaches for commercial data generation tools are actually quite limited in their power, capable of generating data that passes syntactic level checks, like type-checking, and ``not null, unique, primary, and foreign key constraints'' . This is essentially generating data that is database compliant but has little semantic sense.

Random testing is not new~\cite{Hughes:2016:FMB:2896921.2896928}