%introduction

Software testing is a crucial part of systems that run our lives. Industries that rely on predicting consumer habit trends or dispatching taxi cars are deeply affected when their software fails to provide accurate results. The gravity of this trust is even greater in safety-critical programs controlling gas-leak shut off valves or anesthetic delivery machines. Consequently, before these pieces of software are deployed, they must be checked and tested as rigorously as necessary, but no more. The breadth and completeness of testing is directly proportional to how much damage could occur if the software malfunctioned. The software must eventually reach production (used in the hands of the customer or used in industrial practice ``for real'') and cannot spend too much of its lifetime under test. Therefore, the tradeoff of cost and convenience of testing is an important consideration in determining the rigor of testing. Cost is never low, and convenience is never high, so software testing remains an inelegant, ad hoc practice in most industrial applications. Numerous papers agree that ~\cite{Murphy:2007:PRT:1292414.1292425,Haller:2010:TDC:1838126.1838132,Muslu:2015:PDE:2771783.2771792,Tiwari:2013:RRT:2439976.2439982,Gupta:2011:MBA:2002931.2002932,Zeller:2017:STS:3105427.3105438}
. This is not surprising since testing the quality of software is inherently an impossible solution.

Firstly, testing the software means knowing what the software \textit{should} do and what output it \textit{should} generate. A key component of getting correct results is knowledge of what correctness looks like. Developers and testers must be careful not to confuse code that compiles and runs without errors with code that garners accurate results. To find accurate results, testers need an oracle, which is a way of determining how close the actual result of a program is to the expected result. Figure \ref{fig:workflow} illustrates the divergence. Unsurprisingly, attaining and checking this oracle is difficult. Given a particular input to a program, a tester must know what the expected result even is. 
\begin{wrapfigure}{l}{0.65\textwidth}
\centering
\includegraphics[width=90mm,scale=0.5]{diagram.png}
\caption{Basic Workflow of a software program}
\label{fig:workflow}
\end{wrapfigure}
Then, she must know how to read or understand the actual result and be able to compare it to the expected.\footnote{Consequently, many companies often build the role of tester into the role of developer, since the developer has the most intimate knowledge of the design and implementation of the software, and therefore has a better sense of what the output of the program exhibits. Even then, it may be impossible to describe the ideal expected output of the program.} In immensely complex scientific software or finanical programs swathed in accounting terms, the output of the program might be an unknown language to the tester. Even if the tester is a domain scientist, the output may be too complicated to read quickly and identify the trends that indicate she received a postive test result. Lindvall et. al describes this problem in detail when testing autonomous drones.~\cite{Lindvall:2017:MMT:3103620.3103632}

Another major challenge in software testing is the need to know all the possible situations you may want to test for. In the automobile simulation, a tester does not want to just test for braking and accelerating, they might want to test turn-signaling and beeping and brake-lighting, and also combinations of those tests to make sure that the beeping sensor does not accidentally disable the brake pedal capability. Knowing and describing every single situation may be unknown to the tester; they may not even think that that is a situation they have to test for. But these models must be tested. What is known as the ``happy path'', the operational choice that most represents the typical testing scenario, is certainly necessary for testing the program’s reliability, but it does not find the bugs. Outlier situations find the bugs. Identifying all these possible outlier situations is \textbf{Problem One}.

With human's prolific ability to create and store data quantifying the world around them, it is hard to imagine that a tester could not find data that they could use. Gotterbarn agrees that ``Insufficient data is not a problem''~\cite{Gotterbarn:2016:CFC:2874239.2874248}. For example, consider an ocean temperature monitoring software that has predictive power in future local hot spots or cold zones. Ocean temperature data points do exist, and the missing points can likely be interpolated quite easily. But this is a sample size of one. To be sure the temperature projection software is robust, the tester would want to study a variety of circumstances and possibilities – situations that do not even exist. They might perhaps want to study temperature diffusion trends as a result of a hypothetical significant event 30 years from now. Data for that particular test case has to be created.

This is \textbf{Problem Two} -- writing the actual test data itself. A symbolic description of the test must be turned into an actual concrete input to the software. A test vector checking that beeping does not disable braking must be turned into ``beeping=5s\&\&braking=true'' or whatever discrete format the SUT requires. This is not a step that should be done manually. For example, VisIt, a graphical visualization tool~\cite{VisIt}, is capable of handling several gigabyte files. No tester wants to or even could write three gigabytes of data points modeling a real-world situation by hand just to satisfy one test case. Nearly every software testing paper agrees that writing data by hand seriously hinders the testing process~\cite{Misailovic:2007:PTG:1287624.1287645,Murphy:2007:PRT:1292414.1292425,Palka:2011:TOC:1982595.1982615,Patrick:2016:ATI:2970276.2970333}

It is significantly easier to describe the trend of a certain test than it is to create the hundreds (if not more) data points that fit that description. Therefore, the goal of this project is to bridge that gap. It aims to aid the tester in designing and creating test suites so as to provide automation in the testing process. Making a tester's job easier is much more likely to help them do it well, and making this tool accessible and easy to use is a low-cost, high-convenience solution that would codify a standardized testing procedure.

The reality of the software development environment means that engineer must adopt solutions that ensure accurate software while still maintaining a practical timeline and budget.