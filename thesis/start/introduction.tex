%introduction

Software testing is a crucial part of systems that run our lives. Industries that rely on predicting consumer habit trends or dispatching taxi cars are terribly effected when their software fails to provide accurate results. The gravity of this trust is even greater in safety-critical programs controlling gas-leak shut off valves or anesthetic delivery machines. Consequently, before these pieces of software are deployed, they must be checked and tested rigorously. The breadth and completeness of testing is directly proportional to how much damage could occur if the software malfunctioned. The software must eventually reach production (used in the hands of the customer or in industrial practice ``for real'') which means that it has to leave the testing stage eventually. When that time comes, testers must be assured that the software behaves properly. This only sometimes happens. A 2017 report found that 43\% of software engineers did not have enough time to test their code before its release date~\cite{Kassab-deFranco-Laplante}. Therefore, the tradeoff of cost and convenience is an important consideration in determining the rigor of testing. When the release date occurs, testers either have to ship their product without confidence it works or else invest extra time and energy into seeing it does work. Cost is never low, and convenience is never high, so software testing remains a lengthy and sometimes ad hoc practice in many industrial applications. Numerous papers agree that  testing is a tremendously arduous part of software development~\cite{Murphy:2007:PRT:1292414.1292425,Haller:2010:TDC:1838126.1838132,Muslu:2015:PDE:2771783.2771792,Tiwari:2013:RRT:2439976.2439982,Gupta:2011:MBA:2002931.2002932,Zeller:2017:STS:3105427.3105438,Garousi:2017:IWA:3084226.3084264,Kassab-deFranco-Laplante,Langdon:2017:IAT:3105427.3105429,Goffi:2016:AGO:2931037.2931061}. In fact, Misailovic et. al report that 50\% of the software development process is software testing. This is not surprising since testing the quality of software is inherently an impossible solution.

Firstly, testing the software means knowing what the software \textit{should} do and what output it \textit{should} generate. A key component of getting correct results is knowledge of what correctness looks like. Developers and testers must be careful not to confuse code that compiles and runs without errors with code that renders accurate results. To find accurate results, testers need an oracle, which is a way of determining how close the actual result of a program is to the expected result. Figure \ref{fig:workflow} illustrates the divergence. Unsurprisingly, obtaining and checking this oracle is difficult. Given a particular input to a program, a tester must know what the expected result even is. 
\begin{wrapfigure}{l}{0.65\textwidth}
\centering
\includegraphics[width=90mm,scale=0.5]{diagram.png}
\caption{Basic Workflow of a software program}
\label{fig:workflow}
\end{wrapfigure}
Then, she must know how to read or understand the actual result and be able to compare it to the expected.\footnote{Consequently, many companies often build the role of tester into the role of developer, since the developer has the most intimate knowledge of the design and implementation of the software, and therefore has a better sense of what the output of the program exhibits. Even then, it may be impossible to describe the ideal expected output of the program.} In immensely complex scientific software or finanical programs swathed in accounting terms, the output of the program might be an unknown language to the tester. Even if the tester is a domain scientist, the output may be too complicated to read quickly and identify the trends that indicate she received a postive test result.

Another major challenge in software testing is the need to know all the possible situations one may want to test for. In an automobile simulator, for example, a tester does not want to just test for braking and accelerating, she wants to test turn-signaling, beeping, and brake-lighting, and also combinations of those tests to make sure that the beeping sensor does not accidentally disable the brake pedal capability. Knowing and describing every single situation may be unknown to the tester; they may not even think that that is a situation they have to test for. But these situations must be tested. What is known as the ``happy path'', the operational choice that most frequently represents the typical testing scenario, is certainly necessary for testing the program’s reliability, but it does not find the bugs. Outlier situations find the bugs. Lindvall et. al summarize the need for edge cases in their research in testing autonomous drones: ``Just because the drone behaves in a safe manner for a set of test scenarios does not mean that it will always behave in a safe manner for other scenarios''~\cite{Lindvall:2017:MMT:3103620.3103632}. Identifying all these possible outlier situations is \textbf{Problem One}.

With human's prolific ability to create and store data quantifying the world around them, it is hard to imagine that a tester could not find data that they could use. Gotterbarn agrees that ``Insufficient data is not a problem''~\cite{Gotterbarn:2016:CFC:2874239.2874248}. For example, consider an ocean temperature monitoring software that has predictive power in future local hot spots or cold zones. Ocean temperature data points do exist, and the missing points can likely be interpolated quite easily. But this is a sample size of one. To be sure the temperature projection software is robust, the tester would want to study a variety of circumstances and possibilities – situations that do not even exist. They might perhaps want to study temperature diffusion trends as a result of a hypothetical significant event 30 years from now. Data for that particular test case has to be created.

This is \textbf{Problem Two} -- writing the actual test data itself. A symbolic description of the test must be turned into an actual concrete input to the software. A test vector checking that beeping does not disable braking must be turned into ``beeping=5s\&\&braking=true'' or whatever discrete format the SUT requires. This is not a step that should be done manually. For example, VisIt, a graphical visualization tool, is capable of handling several gigabyte files~\cite{VisIt}. No tester wants to or even could write three gigabytes of data points modeling a real-world situation by hand just to satisfy one test case. Nearly every software testing paper agrees that writing data by hand seriously hinders the testing process~\cite{Misailovic:2007:PTG:1287624.1287645,Murphy:2007:PRT:1292414.1292425,Palka:2011:TOC:1982595.1982615,Patrick:2016:ATI:2970276.2970333}.

It is significantly easier to describe the trend of a certain test than it is to create the hundreds (if not more) data points that fit that description. Therefore, the goal of this project is to bridge that gap. It aims to aid the tester in designing and creating test suites so as to provide automation in the testing process. Automation, simplicity, and ease of use are key tenets of this project. Making a tester's job easier is much more likely to help them do it well, and making this tool accessible and easy to use is a low-cost, high-convenience solution that not only codifies a standardized testing procedure but shortens the duration of the testing phase and ensures resilient software before its deployment date. 