%literature survey
One key problem of software testing is knowing what to expect in the output of any given program. The essence of the oracle problem is best captured in the definition of non-testable programs, provided by Weyuker:
\begin{quote}
A program should be considered non-testable [if] (1) there does not exist an oracle; (2) it is theoretically possible, but practically too difficult to determine the correct output~\cite{Chays:2000:FTD:347636.348954}
\end{quote}
There are a variety of reasons why the oracle does not exist or cannot be exercised in a practical capacity. Weyuker describes that some software systems are like magic calculators – they are meant to inform us of the answer. She additionally mentions that some programs produce output that is impossible to read, either because of its volume or complexity. Here lies the core of software testing challenges. It is impossible to solve for every oracle or be able to describe every detail of a complex oracle – every system is entirely different. It goes without saying that this is even more impossible when there are time and resource constraints on the software development timeline. Consequently, a number of researchers have attempted to automate the creation of oracles~\cite{6963470}. For example, one research group used natural-language processing techniques to extract Java comments and design test cases that generate exceptions~\cite{Goffi:2016:AGO:2931037.2931061}. Another group identified and compared outputs of redundancies in software, using them as pseudo-oracles~\cite{Carzaniga:2014:COI:2568225.2568287}. However, the problem of generating complete, automatic oracles remains unsolved. While my project will not make any attempt to create oracles, it will provide information about the input. The test data will be constructed backwards in a sense. Instead of creating random data and then trying to discover its patterns, I will describe a trend and create data that fits that trend, in the hopes that knowing the input's shape will give meaning to the expected output.

Some researchers have done away almost entirely with creating end result oracles by developing techniques specifically used on non-testable programs~\cite{Lindvall:2017:MMT:3103620.3103632,Segura:2017:TAD:3103620.3103626,Chen:2015:MTS:2819261.2819278,Chen:2016:SOC:2970276.2970366,Lindvall:2015:MMT:2819009.2819030}. Metamorphic testing exploits the relationships of different executions of different input data. For example, upon close inspection of a system's implementation, testers can see that outputs should relate directly to their inputs. They may not know much else about the system, or what its output means, but they know what differences they should see upon two different executions. Upon a certain execution they get f(5) = 20, and since they know the input-output relates directly, then they should predict that f(6) $>$ f(5). In and of itself, this is a partial oracle.

Other researchers have proposed methods that do not attempt to provide or calculate an oracle, but rather aid the tester, informed of the structure of the system, exactly what details she should watch and constrain about the oracle~\cite{Patrick:2016:ATI:2970276.2970333,Souza:2016:SMT:2897010.2897012,Harman:2011:SHO:2025113.2025144,Staats:2012:AOC:2337223.2337326}. The idea is that if an existing test suite can find artifical bugs in a system, it not only can find real bugs but also inform the tester what parts of the code must be correct in order for the system not to fail. This approach is called mutation testing. When a test suite finds artifical bugs, it determines which variables in a system are most effective in revealing faults, and then present them to the tester to manually define. This makes the construction of an oracle quite a bit easier.

The problem with metamorphic and mutation testing is that, while they skirt automatic creation of an oracle, they require just as much effort to develop as just creating an oracle by hand. Discovering metamorphic relationships requires a high level of implementation knowledge to write. Mutation testing requires a substantial amount of setup work, and still turns to the tester for manually defined expected values. Even though these techniques are considered cutting-edge testing methods, they do not provide any automation and therefore are not usable enough.

The need for automation is why random testing has developed such popularity. Random testing is not new, but it also is not very useful. Neither \textit{purely} random testing nor purely random data generation is strong enough to adequately test all branches of the code, nor does it provide any useful information about the test case itself or the data it contains. Consequently, constrained randomness, or using randomness in conjunction with other techniques, has gained the benefits of randomness' automation with the structure of other established testing methodologies.

The biggest complaint of pure random testing is that of the distribution type typically implemented: uniform distribution, which is why Lu recommends sampling along sample-space partitions non-uniformly~\cite{Lu:2018:URS:3184558.3186240}, preferring uneven numbers of ``Random Node'' selections and ``Random Edge'' selections. Another generalized technique is that of continous feedback. Arcuri describes Adaptive Random Testing, which monitors the success of the result of a random test case, and then appropriately chooses the next input to be unlike the previous input to attempt to trigger a different result~\cite{Arcuri:2011:ART:2001420.2001452}. All input choices are random, but directed towards certain trends. Arcuri identified a primary problem with this technique in that it often finds repeat bugs and does not extend to as many situations that pure random test cases otherwise could have found. However, several recent  groups similarly use a continuous feedback loops to identify the next test cases but with greater control over random stepping so as to address Arcuri's concerns~\cite{Yatoh:2015:FRT:2771783.2771805,Hughes:2016:FMB:2896921.2896928,Sabor:2015:ART:2819261.2819271}. Finally, one group used random grammar derivations limited to a finite size to generate programs to test compilers~\cite{Palka:2011:TOC:1982595.1982615}. Again this harnesses the automation of random while limiting its recklessness with additional constraints that guide the random selection.

Applying constraints on random data generation is particularly topical to areas of Machine Learning and Database-Driven applications. A team from Columbia University constructed a framework for generating test data for machine-learning models that produce ranking decisions on likely failability of a network of devices. Not only did their framework take in to account the number of examples and number of attributes of a dataset, it considered appearance of repeat values, missing entries, and partitioned categories of examples~\cite{Murphy:2007:PRT:1292414.1292425,Murphy:Kaiser}. However their tool only implemented uniform distribution and could only produce positive integers. This is a keystone feature that indicates the specialization of their framework, and makes extensibility to other applications of software testing nonexistent without significant refactoring. In regards to database-driven applications, there is a considerable lack of developing in testing. The typical approach of data generation is really quite limited in its power. That data really only passes synactic checks, like matching database schema descriptions, and general ``not null, unique, primary, and foreign key constraints''~\cite{Haller:2010:TDC:1838126.1838132}, which indicates that the links between data tables makes sense. The data is database-compliant, it does not have any mistakes, but it contains no semantic meaning. It does not exhibit any trends or patterns. The shortcomings of these applications influence the desired features of my project.
